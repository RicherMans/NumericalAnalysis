
\section{Exercise 1}
%\usepackage{indentfirst}
%\setlength{\parindent}{2em} 
Prove that these facts, needed in the proof of Theorem 2:
\paragraph{a}
If $U$ is invertible and upper triagonal, then $U \adj(U) = I \det(U)$. 
It is now necessary to show that $\adj(U)$ does not change the order of elements within the matrix.
Via definition $\adj(U) = C^T$, where $C_{ji} = (-1)^{i+j} A_{ij}$. $A$ defines the sub matrix which will be generated by removing the $i$th row and the $j$th column.
Moreover $U$ has zero entries for every $i > j$.
Since the entries on the diagonal of $C$ are the same as the ones of $U$ except that at the $i$th row. 
Suppose we remove the $i$th row, so that the new entry at that spot has to be $u_{i+1,i}$. But since $A$ is upper triagonal and $i+1>i$, this guarantees to have at least one 0 entry on the diagonal of $A$. This results in $det(A_{ij}) = 0$ for every $i > j$. Thus $C^T$ is upper triagonal, thus $U^{-1}$ is upper triagonal.

\paragraph{b}


The logic of (a) can be applied in this proof. Just some adjustment for the variables need to be done. Determinates of submatrices are also here zero, which leads to a zero entry above the diagonal where $j >i$.

\paragraph{c}

Assume having an upper triagonal matrix $U$. Since the general matrix multiplication of row $r$ and column $c$ between one and the same matrix $U$ multiplies the $u_{ik} u_{kj}$ element, it needs to be shown that this result is zero for every $i > j$.
\begin{itemize}
\item If $ i > k $ then $u_{ik} = 0$ and therefore $u_{ik} u_{kj} =0 $
\item If $ k > j $ then $u_{kj} = 0$ and therefore $u_{ik} u_{kj} =0 $
\end{itemize}
To conclude the proof, the resulting matrix will be zero if $ i > k $ or the other case if $k>j$. Both cases are covered if $ i > j$.


\section{Exercise 2}

If a matrix exists that the LU decomposition is not unique, then this means that $ A = LU = \hat{L}\hat{U} $
Now that would mean that $L^{-1}\hat{L} = U^{-1}\hat{U}$. Since $L$ and $U$ are lower and upper triagonal, there can only be one possibility, that this equation holds, if both generate the identity matrix $I$. This is the case where $L = \hat{L}$ and $U = \hat{U}$.

\section{Exercise 3}

If $A$ is singular, it would lead to linear dependence, which means that a row will nullify or cancel out another row. In the formulations, the variable $a_{pi}i$ will result in being zero, if linear dependence occurs.
In any other case where $a_{pi}i \neq 0$, we will get a valid result.

\section{Exercise 4}
As already shown before, if $L$ is lower triagonal, then $\adj(L)$ is also lower triagonal. The equation $det(L) I = L \adj(L)$ holds, which means that $L$ is nonsingular, if $det(L)$ is nonzero, whereas otherwise, the diagonal would result in being zero and therefore being singular.

\section{Exercise 6}
The problem displayed in this task is that the matrix A does not have any LU decomposition. One can show that there is no possibility that both, a lower triagonal and an upper triagonal matrix will be generated.
\begin{equation}
A = \left( \begin{array}{ccc}
0 & 1\\
1 & 1\\
\end{array} \right) 
\end{equation}
To display this problem, one could try to find any possibility in which the $1,1$ entry will be generated. Namely there are:
\begin{equation}
L = \left( \begin{array}{ccc}
1 & 0\\
c & d\\
\end{array} \right) 
U = \left( \begin{array}{ccc}
0 & b\\
1 & d\\
\end{array} \right) 
\end{equation}
Here one can clearly see that this is the only valid $A = LU$ decomposition, but this leads to a contradiction, since the Upper Triagonal matrix $U$ has $u_{11} = 0$, which means that this matrix is not a upper triagonal one.

\section{Exercise 7}
In this exercise we need to write down the row and the column version of the doolittle algorithm.
\paragraph{a}
\label{par:a}
Assuming having an matrix, which can be LU decomposed and $a_{i,i} \neq 0$. For the $k$-th row, we obtain the $L$ and $U$ elements by:
\begin{gather*}
l_{k,n} \leftarrow -\frac{a_{k,n}^{(n-1)}}{a_{n,n}^{(n-1)}} \text{ if } k \geq n \\
u_{k,n} \leftarrow a_{k,n} - l_{k,n}  \text{ if } k < n \\
\end{gather*}

\paragraph{b}
The same decomposition as in \ref{par:a} can be done, but by using the inverse indices.

\begin{gather*}
l_{n,k} \leftarrow -\frac{a_{n,k}^{(n-1)}}{a_{n,n}^{(n-1)}} \text{ if } k \leq n \\
u_{n,k} \leftarrow a_{n,k} + l_{n,k}  \text{ if } k > n \\
\end{gather*}

\section{Exercise 8}
In this exercise an algorithm was written to solve equations in the form of $UU^{-1} = I $.
I use the nice property of the upper triagonal matrix, that the dot products when calculating the $k,k$ elements is cancelling every term out except the diagonal ones.
\begin{equation}
(u_{k,k})^{-1} = \frac{1}{u_{k,k}}
\end{equation}
The diagonals can be straight forwardly calculated. The whole algorithm will then calculate the next diagonal elements. To calculate these we need to use the already obtained $k,k$ elements.

The algorithm works as follows, assuming we iterate over $k$ iterations.
\begin{enumerate}
\item Calculate the diagonal $(u_{k,k})^{-1} = \dfrac{1}{u_{k,k}}$
\item Calculate the next level diagonal $(u_{k,p})^{-1} = - \dfrac{\sum\limits_{i=k+1}^{p} \left( u_{k,i} (u_{i,p})^{-1}\right)}{u_{k,k}} $
\item Repeat 1. and 2. until $(u_{1,k})^{-1}$ is calculated
\end{enumerate}
The algorithm finds a result in $O(k^2)$

\section{Exercise 12}
We need to proof that $A$ has an LU decomposition.

\begin{gather*}
A = \left( \begin{array}{cc}
0 & a \\
0 & b \\
\end{array} \right)
= LU = 
\left( \begin{array}{cc}
l_{11} & 0 \\
l_{21} & l_{22} \\
\end{array} \right)
\left( \begin{array}{cc}
u_{11} & u_{12} \\
0 & u_{22} \\
\end{array} \right)
= \\
l_{11} u_{11} = 0 \\
l_{11} u_{12} = a \\
l_{21} u_{11} = 0 \\
l_{21} u_{12} + l_{22} u_{22} = b \\
\end{gather*}
We can see that the equation system cannot be directly solved.
Since there is only one fixed parameter, namely $u_{11} = 0$, I did plug in some reasonable numbers into the equations to get an intuition about the final matrices.
\begin{gather*}
l_{11} u_{12} = a \rightarrow  u_{11} = 0 \\
l_{21} u_{12} + \ldots = b \rightarrow  u_{12} \wedge l_{11} = a , \rightarrow u_{12} = a , l_{11} = 1 \\
l_{21} = 0 \\
u_{22} \wedge l_{22} = b \rightarrow u_{22} = b, l_{22} = 1 \\
L =
\left( \begin{array}{cc}
1 & 0 \\
0 & 1 \\
\end{array} \right)
U =
\left( \begin{array}{cc}
0 & a\\
0 & b 
\end{array} \right)
\end{gather*}
As it can be seen, this factorization is not unique e.g. one can interchange the $L$ and $U$ elements in $l_{22}$ with $u_{22}$ and get a new matrix.
Now I will proof that assuming a unit matrix, it doesn't change the behaviour of the system.
\begin{gather*}
A = \left( \begin{array}{cc}
0 & a \\
0 & b \\
\end{array} \right)
= LU = 
\left( \begin{array}{cc}
1 & 0 \\
l_{21} & 1 \\
\end{array} \right)
\left( \begin{array}{cc}
u_{11} & u_{12} \\
0 & u_{22} \\
\end{array} \right)
= \\
u_{11} = 0 \\
u_{12} = a   \\
l_{21} u_{11} = 0 \\
l_{21} u_{12} + u_{22} = b \rightarrow u_{22} = b - l_{21} a \rightarrow l_{21} = 0\\
\end{gather*}
We can see that assuming a unit lower triagonal matrix, will make all the values within the matrix unique and we obtain only one solution.
It should be noted that as long $b \neq n a$, we can even obtain any solution, since in the case of $b = n a$, the matrix would be singular.

\section{Exercise 13}
We need to proof that $A$ has an LU decomposition.

\begin{gather*}
A = \left( \begin{array}{cc}
0 & 0 \\
a & b \\
\end{array} \right)
= LU = 
\left( \begin{array}{cc}
l_{11} & 0 \\
l_{21} & l_{22} \\
\end{array} \right)
\left( \begin{array}{cc}
u_{11} & u_{12} \\
0 & u_{22} \\
\end{array} \right)
\end{gather*}
Which leads to following linear equations:
\begin{gather*}
l_{11} u_{11} = 0 \\
l_{11} u_{12} = 0 \\
l_{21} u_{11} = a  \rightarrow l_{11} = 0, l_{21} \wedge u_{11} = a\\
l_{21} u_{12} + l_{22} u_{22} = b \rightarrow l_{22} \wedge u_{22} = b\\
\end{gather*}
Again in this equation system it is under determined, so we cannot obtain a solution for every variable.
If we now set $L$ to be unit triagonal, we get:
\begin{gather*}
A = \left( \begin{array}{cc}
0 & 0 \\
a & b \\
\end{array} \right)
= LU = 
\left( \begin{array}{cc}
1 & 0 \\
l_{21} & 1 \\
\end{array} \right)
\left( \begin{array}{cc}
u_{11} & u_{12} \\
0 & u_{22} \\
\end{array} \right)
\end{gather*}
\begin{gather*}
\label{3}
u_{11} = 0  \\
u_{12} = 0 \\
l_{21} u_{11} = a \\
l_{21} u_{12} + u_{22} = b
\end{gather*}
Yet again in this case, the equations will follow to a contradiction, so that $A$ has no $LU$ factorization. We can see that $l_{21} u_{11} = a$, where $u_{11}$, so that this equation system has no $LU$ factorization.

\section{Exercise 15}
Find all factorizations of $A$ which are unit lower triagonal.
\begin{gather*}
A = \left( \begin{array}{cc}
1 & 5 \\
3 & 15 
\end{array}
\right)
= LU =
\left( \begin{array}{cc}
1 & 0 \\
l_{21} & 1 
\end{array} \right)
\left( \begin{array}{cc}
u_{11} & u_{12} \\
0 & u_{22} 
\end{array} \right)
\\
u_{11} = 1 \\
u_{12} = 5 \\
l_{21} u_{11} = 3 \rightarrow l_{21} = 3 \\
u_{12} l_{21} + u_{22} = 15 \rightarrow u_{22} = 0
\end{gather*}
It can be seen that $A$ has only one $LU$ factorization.
\begin{gather*}
LU =
\left( \begin{array}{cc}
1 & 0 \\
3 & 1 
\end{array} \right)
\left( \begin{array}{cc}
1 & 5 \\
0 & 0 
\end{array} \right)
\end{gather*}
\section{Exercise 16}
If A is invertible and has an LU decomposition then all principal minors of $A$ are non-singular.
Given $a_{11} \neq 0$ in $A$ so that we can transform this matrix $A$ with Gaussian elimination into a $LU$ form so that $l_{i,i} \neq 0$ and $\det(A) = \sum\limits_i u_{ii}$.
After $k$ steps of Gaussian elimination $A^k$, we can see that the sub matrix $1:k-1,1:k-1$ is unit lower triagonal, so per definition its determinate is non-zero: $det(A{1:k-1,1:k-1}) = \sum\limits_i a_{ii} = 1 \neq 0 $. Therefore the $k$-th pivot is non-zero, so we can proceed to find a $LU$ decomposition.

If $L$ is unit lower triagonal $A = LU$ then $\det(A) = \det(LU) = \det(L) \det(U) = \sum\limits_i u_{ii}$.

\section{Exercise 19}
If the unit triangular LU decomposition exists so that:
\begin{gather*}
LU = 
\left( \begin{array}{cc}
1 & 0 \\
l_{21} & 1 \\
\end{array} \right)
\left( \begin{array}{cc}
u_{11} & u_{12} \\
0 & u_{22} \\
\end{array} \right)
\end{gather*}
We want to know if this equation can also state that a unit upper triangular matrix can be generated.
To distinguish the two different matrices, I use a superscript to denote if it's the lower unit triangular (superscript $1$ ) or $2$ for the upper triangular.
\begin{gather*}
LU = 
\left( \begin{array}{cc}
1 & 0 \\
l^1_{21} & 1 \\
\end{array} \right)
\left( \begin{array}{cc}
u^1_{11} & u^1_{12} \\
0 & u^1_{22} \\
\end{array} \right)
=
\left( \begin{array}{cc}
l^2_{11} & 0 \\
l^2_{21} & l^2_{22} \\
\end{array} \right)
\left( \begin{array}{cc}
1 & u ^2_{12} \\
0 & 1 \\
\end{array} \right)
\end{gather*}
This gives us following equations:
\begin{gather*}
u^1_{11} = l^2_{11}  \\
u^1_{12} = l^2_{11} u^2_{12} \\
l^{1}_{21} u^{1}_{11} = l^{2}_{21} \\ 
l^1_{12} u^1_{12} + u^1_{22} = l^2_{21} u^2_{12} + l^2_{22}
\end{gather*}
This leads to the following terms, in respect to the $2$ superscript terms:
\begin{gather*}
l^2_{11} = u^1_{11}
u^2_{12} = \frac{u^1_{12}}{u^1_{11}} \\
l^2_{21} = l^1_{21} u^1_{11} \\
l^2_{22} = \frac{l^1_{21} u^1_{12} + u^1_{22}}{l^2_{21} u^2_{12}} = 1 + \frac{u^1_{22}}{l^1_{21} u^1_{12}} \\
\end{gather*}

That is, if we write down (the terms denoted by superscript $2$) in the matrix form:
\begin{gather*}
\left( \begin{array}{cc}
u^1_{11} & 0 \\
l^1_{21} u^1_{11} & 1 + \dfrac{u^1_{22}}{l^1_{21} u^1_{12}} \\
\end{array} \right)
\left( \begin{array}{cc}
1 & \dfrac{u^1_{12}}{u^1_{11}} \\
0 & 1 \\
\end{array} \right)
\end{gather*}
So it is seen that an unit lower triangular can be transformed into an upper triangular matrix. 

\section{Exercise 24}
We need to show that an invertible matrix $A$ can be factorized into a $LDU$ decomposition.

Assume that having an transformed matrix $U^{'}$, which has $u^{'}_{ii} = 1$. In that way we can use a diagonal matrix $D$ to restore the missing values, by stetting $d_{ii} = u_{ii}$.
\begin{gather*}
A = (LDU^{'})^{T^{T}} = (U^{{'}^T} D L^T)^{T}
\end{gather*}
As we can see, the matrix $U^{{'}^T}$  is a lower triangular one and the matrix $L^T$ is a upper triangonal one. This means that $LDU^{'}$ is just another $LU$ decomposition of $A$. We can set $L=U^{{'}^T}$ so that we can see that this decomposition is unique:
\begin{gather*}
A = {(L D L^T)}^T
\end{gather*}
This shows that the $LDU$ decomposition of $A$ can be found.

\section{Exercise 29}

In this exercise a $LDL^T$ transformation needs to be done.
\begin{gather*}
A = \left( \begin{array}{ccc}
2 & 6 & -4\\
6 & 17 & -17 \\
-4 & -17 & -20 
\end{array} \right) 
= LDL^T = 
\left( \begin{array}{ccc}
1 & 0 & 0 \\
l_{21} & 1 & 0 \\
l_{31} & l_{32} & 1
\end{array} \right)
\left( \begin{array}{ccc}
d_1 & 0 & 0 \\
0 & d_{2} & 0 \\
0 & 0 & d_{3}
\end{array} \right)
\left( \begin{array}{ccc}
1 & l_{21} & l_{31} \\
0 & 1 & l_{32} \\
0 & 0 & 1
\end{array} \right)
\end{gather*}
We will write out the resulting equations.
\begin{gather*}
d_1 = 2 \\
l_{21} d_1 = 6 \rightarrow l_{21} = 3 \\
d_{2} = 17 \\
l_{31} d_1 = -4 \rightarrow l_{31} = -2\\
l_{32} d_2 = -17 \rightarrow l_{32} = -1\\
d_3 = -20
\end{gather*}
Which will result in the following matrices:
\begin{gather*}
A = \left( \begin{array}{ccc}
2 & 6 & -4\\
6 & 17 & -17 \\
-4 & -17 & -20 
\end{array} \right) 
= LDL^T = 
\left( \begin{array}{ccc}
1 & 0 & 0\\
3 & 1 & 0 \\
-2 & -1 & 1 
\end{array} \right)
\left( \begin{array}{ccc}
2 & 0 & 0\\
0 & 17 & 0\\
0 & 0 & -20
\end{array} \right)
\left( \begin{array}{ccc}
1 & 3 & -2 \\
0 & 1 & -1 \\
0 & 0 & 1
\end{array} \right)
\end{gather*}

\section{Exercise 31}
In this exercise I use scaled row pivoting to compute the $LU$ decomposition.
\begin{gather*}
A  = \left( \begin{array}{ccc}
3 & 0 & 1 \\
0 & -1 & 3 \\
1 & 3 & 0
\end{array} \right) = \\
\left( \begin{array}{ccc}
3 & 0 & 0\\
0 & -1 & 3 \\
\frac{1}{3} & 3 & -\frac{1}{3}
\\
\end{array} \right)
\left( \begin{array}{ccc}
3 & 0 & 1\\
0 & -\frac{1}{3} & \frac{26}{9} \\
\frac{1}{3} & 3 & -\frac{1}{3}
\end{array} \right)
\\
\text{Matrices here are already permutated by P: } \\
LU = 
\left( \begin{array}{ccc}
1 & 0 & 0 \\
\frac{1}{3} & 1 & 0 \\
0 & -\frac{1}{3} & 1
\end{array} \right)
\left( \begin{array}{ccc}
3 & 0 & 1\\
0 & 3 & -\frac{1}{3}\\
0 & 0 & -\frac{26}{9}
\end{array} \right)
\\
P = 
\left( \begin{array}{ccc}
1 & 0 & 0\\
0 & 0 & 1 \\
0 & 1 & 0
\end{array} \right)
\end{gather*}
